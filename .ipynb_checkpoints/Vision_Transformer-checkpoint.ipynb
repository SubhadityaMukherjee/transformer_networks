{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf14be6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Mostly from https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial15/Vision_Transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02cbd8a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17012/4055705205.py:26: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats(\"svg\", \"pdf\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOADING EVERYTHING\n",
    "from src.models import *\n",
    "from src.blocks import *\n",
    "from src.utils import *\n",
    "\n",
    "from torchvision.datasets import *\n",
    "from torchvision import transforms\n",
    "from jax import random\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import jax\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "plt.set_cmap(\"cividis\")\n",
    "\n",
    "set_matplotlib_formats(\"svg\", \"pdf\")\n",
    "\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1dbd956",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: gpu:0\n"
     ]
    }
   ],
   "source": [
    "# SETTING UP DEFAULTS\n",
    "DATASET_PATH = \"/media/hdd/Datasets\"\n",
    "CHECKPOINT_PATH = \"saved_models/viTJax\"\n",
    "valid_size = 0.2\n",
    "main_rng = random.PRNGKey(42)\n",
    "\n",
    "print(\"Device:\", jax.devices()[0])\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "DATA_MEANS = np.array([0.49139968, 0.48215841, 0.44653091])\n",
    "DATA_STD = np.array([0.24703223, 0.24348513, 0.26158784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2122b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA TRANSFORMS\n",
    "def image_to_numpy(img):\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = (img / 255.0 - DATA_MEANS) / DATA_STD\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2126e22-099c-4ffd-8840-648b8b4d9a9a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_transform = image_to_numpy\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "        image_to_numpy,\n",
    "    ]\n",
    ")\n",
    "ds_name = CIFAR10\n",
    "train_dataset = ds_name(\n",
    "    root=DATASET_PATH, train=True, transform=train_transform, download=True\n",
    ")\n",
    "val_dataset = ds_name(\n",
    "    root=DATASET_PATH, train=True, transform=test_transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "623fcaec-5602-4ffa-9ec4-a2016f087f51",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# train_set, _ = torch.utils.data.random_split(\n",
    "#     train_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42),\n",
    "# )\n",
    "# _, val_set = torch.utils.data.random_split(\n",
    "#     val_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42), \n",
    "# )\n",
    "\n",
    "test_set = ds_name(\n",
    "    root=DATASET_PATH, train=False, transform=test_transform, download=True\n",
    ")\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb6bc83",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# DATA LOADERS\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=numpy_collate,\n",
    "    num_workers=8,\n",
    "    persistent_workers=True,\n",
    "    sampler=train_sampler,\n",
    ")\n",
    "val_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    "    collate_fn=numpy_collate,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    "    sampler=valid_sampler,\n",
    ")\n",
    "test_loader = data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=numpy_collate,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "589fe9e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# VISUALIZING THINGS\n",
    "if not Path.exists(Path(\"outputs\")):\n",
    "    os.mkdir(\"outputs\")\n",
    "NUM_IMAGES = 4\n",
    "CIFAR_images = np.stack([test_set[idx][0] for idx in range(NUM_IMAGES)], axis=0)\n",
    "img_grid = torchvision.utils.make_grid(\n",
    "    numpy_to_torch(CIFAR_images), nrow=4, normalize=True, pad_value=0.9\n",
    ")\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Image examples of the CIFAR10 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"outputs/ViT-image-examples.png\", dpi=200)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2366bcd-bbde-494d-8752-8370ed5bfca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_patches = img_to_patch(CIFAR_images, patch_size=4, flatten_channels=False)\n",
    "\n",
    "fig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14, 3))\n",
    "fig.suptitle(\"Images as input sequences of patches\")\n",
    "for i in range(CIFAR_images.shape[0]):\n",
    "    img_grid = torchvision.utils.make_grid(\n",
    "        numpy_to_torch(img_patches[i]), nrow=64, normalize=True, pad_value=0.9\n",
    "    )\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "    ax[i].imshow(img_grid)\n",
    "    ax[i].axis(\"off\")\n",
    "#  plt.show()\n",
    "\n",
    "plt.savefig(\"outputs/ViT-patches.png\", dpi=200)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5da2f11-f3d5-4101-be1d-b010a3ee2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        CHECKPOINT_PATH,\n",
    "        exmp_imgs,\n",
    "        lr=1e-3,\n",
    "        weight_decay=0.01,\n",
    "        seed=42,\n",
    "        **model_hparams\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Module for summarizing all training functionalities for classification on CIFAR10.\n",
    "\n",
    "        Inputs:\n",
    "            exmp_imgs - Example imgs, used as input to initialize the model\n",
    "            lr - Learning rate of the optimizer to use\n",
    "            weight_decay - Weight decay to use in the optimizer\n",
    "            seed - Seed to use in the model initialization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.seed = seed\n",
    "        self.rng = jax.random.PRNGKey(self.seed)\n",
    "\n",
    "        self.model = model(**model_hparams)\n",
    "        self.CHECKPOINT_PATH = CHECKPOINT_PATH\n",
    "\n",
    "        self.log_dir = os.path.join(self.CHECKPOINT_PATH, \"logs\")\n",
    "        self.logger = SummaryWriter(log_dir=self.log_dir)\n",
    "\n",
    "        self.create_functions()\n",
    "\n",
    "        self.init_model(exmp_imgs)\n",
    "        self.loss_log = []\n",
    "        self.metric_log = []\n",
    "\n",
    "    def create_functions(self):\n",
    "        def calculate_loss(params, rng, batch, train):\n",
    "            imgs, labels = batch\n",
    "            labels_onehot = jax.nn.one_hot(labels, num_classes=self.model.num_classes)\n",
    "            rng, dropout_apply_rng = random.split(rng)\n",
    "            logits = self.model.apply(\n",
    "                {\"params\": params},\n",
    "                imgs,\n",
    "                train=train,\n",
    "                rngs={\"dropout\": dropout_apply_rng},\n",
    "            )\n",
    "            loss = optax.softmax_cross_entropy(logits, labels_onehot).mean()\n",
    "            acc = (logits.argmax(axis=-1) == labels).mean()\n",
    "            return loss, (acc, rng)\n",
    "\n",
    "        def train_step(state, rng, batch):\n",
    "            def loss_fn(params):\n",
    "                return calculate_loss(params, rng, batch, train=True)\n",
    "\n",
    "            (loss, (acc, rng)), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n",
    "                state.params\n",
    "            )\n",
    "\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "            return state, rng, loss, acc\n",
    "\n",
    "        def eval_step(state, rng, batch):\n",
    "\n",
    "            _, (acc, rng) = calculate_loss(state.params, rng, batch, train=False)\n",
    "            return rng, acc\n",
    "\n",
    "        self.train_step = jax.jit(train_step)\n",
    "        self.eval_step = jax.jit(eval_step)\n",
    "\n",
    "    def init_model(self, exmp_imgs):\n",
    "\n",
    "        self.rng, init_rng, dropout_init_rng = random.split(self.rng, 3)\n",
    "        self.init_params = self.model.init(\n",
    "            {\"params\": init_rng, \"dropout\": dropout_init_rng}, exmp_imgs, train=True\n",
    "        )[\"params\"]\n",
    "        self.state = None\n",
    "\n",
    "    def init_optimizer(self, num_epochs, num_steps_per_epoch):\n",
    "\n",
    "        lr_schedule = optax.piecewise_constant_schedule(\n",
    "            init_value=self.lr,\n",
    "            boundaries_and_scales={\n",
    "                int(num_steps_per_epoch * num_epochs * 0.6): 0.1,\n",
    "                int(num_steps_per_epoch * num_epochs * 0.85): 0.1,\n",
    "            },\n",
    "        )\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(1.0),  # Clip gradients at norm 1\n",
    "            optax.adamw(lr_schedule, weight_decay=self.weight_decay),\n",
    "        )\n",
    "\n",
    "        self.state = train_state.TrainState.create(\n",
    "            apply_fn=self.model.apply,\n",
    "            params=self.init_params if self.state is None else self.state.params,\n",
    "            tx=optimizer,\n",
    "        )\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, num_epochs=200,graph_progress=None):\n",
    "        self.train_loader = train_loader\n",
    "\n",
    "        self.init_optimizer(num_epochs, len(self.train_loader))\n",
    "\n",
    "        best_eval = 0.0\n",
    "        tq = tqdm(range(1, num_epochs + 1))\n",
    "        for epoch_idx in tq:\n",
    "            self.train_epoch(epoch=epoch_idx)\n",
    "            if epoch_idx % 2 == 0:\n",
    "                eval_acc = self.eval_model(val_loader)\n",
    "                self.logger.add_scalar(\"val/acc\", eval_acc, global_step=epoch_idx)\n",
    "                tq.set_postfix({\"val/acc\": eval_acc})\n",
    "                if eval_acc >= best_eval:\n",
    "                    best_eval = eval_acc\n",
    "                    self.save_model(step=epoch_idx)\n",
    "                self.logger.flush()\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "\n",
    "        metrics = defaultdict(list)\n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\", leave=False):\n",
    "            self.state, self.rng, loss, acc = self.train_step(\n",
    "                self.state, self.rng, batch\n",
    "            )\n",
    "            metrics[\"loss\"].append(loss)\n",
    "            metrics[\"acc\"].append(acc)\n",
    "        for key in metrics:\n",
    "            avg_val = np.stack(jax.device_get(metrics[key])).mean()\n",
    "            self.logger.add_scalar(\"train/\" + key, avg_val, global_step=epoch)\n",
    "\n",
    "    def eval_model(self, data_loader):\n",
    "\n",
    "        correct_class, count = 0, 0\n",
    "        for batch in data_loader:\n",
    "            self.rng, acc = self.eval_step(self.state, self.rng, batch)\n",
    "            correct_class += acc * batch[0].shape[0]\n",
    "            count += batch[0].shape[0]\n",
    "        eval_acc = (correct_class / count).item()\n",
    "        return eval_acc\n",
    "\n",
    "    def save_model(self, step=0):\n",
    "\n",
    "        checkpoints.save_checkpoint(\n",
    "            ckpt_dir=self.log_dir, target=self.state.params, step=step, overwrite=True\n",
    "        )\n",
    "\n",
    "    def load_model(self, name=\"ViT.ckpt\", pretrained=False):\n",
    "\n",
    "        if not pretrained:\n",
    "            params = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=None)\n",
    "        else:\n",
    "            params = checkpoints.restore_checkpoint(\n",
    "                ckpt_dir=os.path.join(self.CHECKPOINT_PATH, name), target=None\n",
    "            )\n",
    "        self.state = train_state.TrainState.create(\n",
    "            apply_fn=self.model.apply,\n",
    "            params=params,\n",
    "            tx=self.state.tx if self.state else optax.adamw(self.lr),\n",
    "        )\n",
    "\n",
    "    def checkpoint_exists(self, name=\"ViT.ckpt\"):\n",
    "        return os.path.isfile(os.path.join(self.CHECKPOINT_PATH, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87cef037-ff3d-4c7d-9a10-9f7694bff965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL TRAINING\n",
    "def train_model(*args, num_epochs=200, retrain=False,graph_progress=None, **kwargs):\n",
    "    trainer = TrainerModule(*args, **kwargs)\n",
    "    if not trainer.checkpoint_exists() or retrain == True:\n",
    "        print(\"Training\")\n",
    "        trainer.train_model(train_loader, val_loader, num_epochs=num_epochs, graph_progress=graph_progress)\n",
    "        trainer.load_model()\n",
    "    else:\n",
    "        print(\"Skipping training\")\n",
    "        trainer.load_model(pretrained=True)\n",
    "    val_acc = trainer.eval_model(val_loader)\n",
    "    test_acc = trainer.eval_model(test_loader)\n",
    "    return trainer, {\"val\": val_acc, \"test\": test_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4297f709-5a33-4c2f-8e6a-0634d209adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.0000000e+00  6.8095636e-01  9.4292098e-01 ...  5.9464777e-01\n",
      "    9.7415698e-01  1.0585586e+00]\n",
      "  [ 1.7232538e+00 -4.9299723e-01 -6.9160980e-01 ...  1.1262830e-01\n",
      "    0.0000000e+00 -3.3333950e+00]\n",
      "  [ 2.2330794e+00 -1.6634367e+00  1.7566570e+00 ... -1.7800977e+00\n",
      "   -2.3057868e+00 -4.3361320e+00]\n",
      "  ...\n",
      "  [ 0.0000000e+00  3.0686074e-01 -6.2978923e-01 ...  4.1924715e-01\n",
      "   -3.4807259e-01  0.0000000e+00]\n",
      "  [ 0.0000000e+00 -7.8523552e-01 -3.6564264e-01 ...  2.1614246e+00\n",
      "    7.4168545e-01 -1.2719804e-01]\n",
      "  [ 1.0251516e+00  5.7626343e-01 -6.1975475e-02 ... -1.4406445e+00\n",
      "   -1.3035455e+00  1.1814802e+00]]\n",
      "\n",
      " [[ 5.4608119e-01  7.8261006e-01  6.1881483e-01 ...  7.3245621e-01\n",
      "    1.1697980e+00  1.5483885e+00]\n",
      "  [-4.2444515e-01 -5.0661508e-03 -1.4525530e+00 ...  2.9910266e+00\n",
      "    1.5871063e+00 -4.3330958e-01]\n",
      "  [ 2.0300534e-01 -1.3390758e+00 -3.4605920e-01 ...  2.8487520e+00\n",
      "   -2.7288872e-01  1.6185313e-02]\n",
      "  ...\n",
      "  [ 5.6304932e-02  7.7889703e-02 -5.4451352e-01 ...  3.9343886e-02\n",
      "    1.2777460e+00  1.4566622e+00]\n",
      "  [-9.8432171e-01 -1.1717975e+00 -1.6559136e+00 ...  2.8371022e+00\n",
      "    1.0025311e+00  2.2463286e-01]\n",
      "  [-3.0625097e-02  2.9649961e-01 -2.7128696e-02 ... -7.7085143e-01\n",
      "   -5.4356939e-01  1.4430385e+00]]\n",
      "\n",
      " [[ 4.3313468e-01  6.8095636e-01  8.5256338e-01 ... -5.7747519e-01\n",
      "    8.5804397e-01  9.6392918e-01]\n",
      "  [-2.4948509e+00 -2.5675848e-01 -4.4310098e+00 ...  3.6234674e+00\n",
      "    2.5060079e+00 -5.5180734e-01]\n",
      "  [-3.5456094e-01 -2.9523513e-01 -2.1429112e+00 ...  2.5257847e+00\n",
      "   -7.3901430e-02 -4.1394114e-02]\n",
      "  ...\n",
      "  [ 1.9045511e+00  0.0000000e+00  1.3686359e+00 ... -1.1851674e+00\n",
      "    2.7414903e-01 -8.8209224e-01]\n",
      "  [ 0.0000000e+00 -1.3152812e+00 -5.8361816e-01 ... -3.7586391e-01\n",
      "    1.7608895e+00 -6.5320551e-01]\n",
      "  [-4.0241835e-01  7.8533304e-01 -1.6762346e+00 ...  9.2971765e-02\n",
      "   -1.6075177e-01  1.8460960e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 7.8795350e-01  6.8095636e-01  8.1060731e-01 ...  1.8780867e-02\n",
      "    1.2707319e+00  1.2334105e+00]\n",
      "  [-1.1040963e+00  5.8896685e-01  1.3692640e-01 ...  0.0000000e+00\n",
      "    1.4021055e+00  5.5540961e-01]\n",
      "  [ 4.4705731e-01 -7.1577281e-01 -4.2369968e-01 ...  3.1849275e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  ...\n",
      "  [ 7.4474937e-01 -1.7451054e-01 -1.0343924e+00 ... -1.2179662e+00\n",
      "    1.1305014e+00  1.5145949e-02]\n",
      "  [-5.3092539e-01 -1.1679026e+00  0.0000000e+00 ...  2.2848254e-01\n",
      "   -5.8125138e-02 -8.8514584e-01]\n",
      "  [ 7.4105698e-01 -2.8481361e-01 -7.5931621e-01 ...  0.0000000e+00\n",
      "    1.4940365e-01  2.1723376e-01]]\n",
      "\n",
      " [[ 2.0831576e-01  9.2587346e-01  1.1008787e+00 ...  6.8165284e-01\n",
      "    8.1425321e-01  7.0859802e-01]\n",
      "  [ 1.1390471e+00 -1.1400647e+00 -8.2473159e-02 ... -2.0378897e+00\n",
      "    6.3145149e-01 -4.7930455e+00]\n",
      "  [ 0.0000000e+00  2.0000023e-01  1.7157531e+00 ... -2.9536581e+00\n",
      "   -1.4949119e+00 -5.1483808e+00]\n",
      "  ...\n",
      "  [-2.1637268e-01  4.2576957e-01 -2.8543866e+00 ... -9.4862509e-01\n",
      "    1.4900087e+00  7.8940547e-01]\n",
      "  [ 0.0000000e+00 -1.5172482e+00 -3.0427659e-01 ... -4.0022872e-02\n",
      "    9.6082896e-01 -2.4574738e+00]\n",
      "  [ 7.8248870e-01  1.1604413e-02 -5.3189802e-01 ... -4.1249638e+00\n",
      "   -5.3742945e-01 -1.2829319e+00]]\n",
      "\n",
      " [[ 9.1493595e-01 -2.2901559e-01  9.2560929e-01 ...  0.0000000e+00\n",
      "   -1.7042799e-01  1.3015532e+00]\n",
      "  [-1.3222371e+00 -8.1224576e-02 -3.0018160e+00 ...  1.7988799e+00\n",
      "    2.2459331e+00  9.6931756e-02]\n",
      "  [ 3.2660559e-01  0.0000000e+00 -1.5168797e+00 ...  2.0838401e+00\n",
      "    4.6463735e-02 -7.3424518e-02]\n",
      "  ...\n",
      "  [ 2.2251692e-01  5.5161104e-02 -9.0914303e-01 ... -6.6959327e-01\n",
      "    1.0316812e+00  9.8692834e-02]\n",
      "  [-1.4358745e+00 -7.4032867e-01  2.3541370e-01 ... -6.3599542e-02\n",
      "    2.5699589e+00  5.7057649e-01]\n",
      "  [-2.5674877e-01 -1.6217476e-01 -8.0421728e-01 ... -2.1409020e+00\n",
      "    9.5069110e-01  5.2206415e-01]]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexmp_imgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVisionTransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT results\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs, retrain, graph_progress, *args, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\u001b[38;5;241m*\u001b[39margs, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, retrain\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,graph_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m----> 3\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainerModule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mcheckpoint_exists() \u001b[38;5;129;01mor\u001b[39;00m retrain \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mTrainerModule.__init__\u001b[0;34m(self, model, CHECKPOINT_PATH, exmp_imgs, lr, weight_decay, seed, **model_hparams)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m SummaryWriter(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_functions()\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexmp_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_log \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_log \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mTrainerModule.init_model\u001b[0;34m(self, exmp_imgs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, exmp_imgs):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng, init_rng, dropout_init_rng \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_init_rng\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexmp_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m/media/hdd/github/transformer_networks/src/models.py:68\u001b[0m, in \u001b[0;36mVisionTransformer.__call__\u001b[0;34m(self, x, train)\u001b[0m\n\u001b[1;32m     66\u001b[0m     x,raw_sc \u001b[38;5;241m=\u001b[39m attn_block(x, train\u001b[38;5;241m=\u001b[39mtrain)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(raw_sc)\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_attentions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(raw_sc)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     71\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_head(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "model, results = train_model(\n",
    "    exmp_imgs=next(iter(train_loader))[0],\n",
    "    embed_dim=256,\n",
    "    hidden_dim=512,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    patch_size=4,\n",
    "    num_channels=3,\n",
    "    num_patches=64,\n",
    "    num_classes=10,\n",
    "    dropout_prob=0.2,\n",
    "    lr=3e-4,\n",
    "    retrain=True,\n",
    "    num_epochs=1,\n",
    "    CHECKPOINT_PATH=CHECKPOINT_PATH,\n",
    "    model=VisionTransformer,\n",
    "    graph_progress=10\n",
    ")\n",
    "print(\"ViT results\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1878d6-b790-496d-8469-cf8237297ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 256\n",
    "loader = transforms.Compose([transforms.Resize(imsize)])\n",
    "from PIL import Image\n",
    "def image_loader(image_name):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = Image.open(image_name)\n",
    "    image = jax.numpy.array(loader(image))\n",
    "    return image  #assumes that you're using GPU\n",
    "test_im = image_loader(\"/media/hdd/Datasets/boat/cruise ship/adventure-of-the-seas-cruise-ship-caribb-1218316.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba41e19b-7fa3-4b31-92d0-dff69eeb6e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15452a6-d993-4058-9355-0c163eb48e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ba71c-9765-41d2-89d5-0b4ac3232788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f1aba2-8597-4276-a543-1c81f675114a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e825b9-eb13-4215-a7b0-a134a5b54c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b847ac3-2311-4496-a13f-987c81bbe10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9376fa9-7f6d-4bb8-9b7b-a18638681eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

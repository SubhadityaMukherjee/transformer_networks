{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf14be6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Mostly from https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial15/Vision_Transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f02cbd8a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5641/4055705205.py:26: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats(\"svg\", \"pdf\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOADING EVERYTHING\n",
    "from src.models import *\n",
    "from src.blocks import *\n",
    "from src.utils import *\n",
    "\n",
    "from torchvision.datasets import *\n",
    "from torchvision import transforms\n",
    "from jax import random\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import jax\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "plt.set_cmap(\"cividis\")\n",
    "\n",
    "set_matplotlib_formats(\"svg\", \"pdf\")\n",
    "\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1dbd956",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: gpu:0\n"
     ]
    }
   ],
   "source": [
    "# SETTING UP DEFAULTS\n",
    "DATASET_PATH = \"/media/hdd/Datasets\"\n",
    "CHECKPOINT_PATH = \"saved_models/viTJax\"\n",
    "valid_size = 0.2\n",
    "main_rng = random.PRNGKey(42)\n",
    "\n",
    "print(\"Device:\", jax.devices()[0])\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "DATA_MEANS = np.array([0.49139968, 0.48215841, 0.44653091])\n",
    "DATA_STD = np.array([0.24703223, 0.24348513, 0.26158784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2122b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA TRANSFORMS\n",
    "def image_to_numpy(img):\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = (img / 255.0 - DATA_MEANS) / DATA_STD\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2126e22-099c-4ffd-8840-648b8b4d9a9a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_transform = image_to_numpy\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "        image_to_numpy,\n",
    "    ]\n",
    ")\n",
    "ds_name = CIFAR10\n",
    "train_dataset = ds_name(\n",
    "    root=DATASET_PATH, train=True, transform=train_transform, download=True\n",
    ")\n",
    "val_dataset = ds_name(\n",
    "    root=DATASET_PATH, train=True, transform=test_transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "623fcaec-5602-4ffa-9ec4-a2016f087f51",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# train_set, _ = torch.utils.data.random_split(\n",
    "#     train_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42),\n",
    "# )\n",
    "# _, val_set = torch.utils.data.random_split(\n",
    "#     val_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42), \n",
    "# )\n",
    "\n",
    "test_set = ds_name(\n",
    "    root=DATASET_PATH, train=False, transform=test_transform, download=True\n",
    ")\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fb6bc83",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# DATA LOADERS\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=numpy_collate,\n",
    "    num_workers=8,\n",
    "    persistent_workers=True,\n",
    "    sampler=train_sampler,\n",
    ")\n",
    "val_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    "    collate_fn=numpy_collate,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    "    sampler=valid_sampler,\n",
    ")\n",
    "test_loader = data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=numpy_collate,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "589fe9e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# VISUALIZING THINGS\n",
    "if not Path.exists(Path(\"outputs\")):\n",
    "    os.mkdir(\"outputs\")\n",
    "NUM_IMAGES = 4\n",
    "CIFAR_images = np.stack([test_set[idx][0] for idx in range(NUM_IMAGES)], axis=0)\n",
    "img_grid = torchvision.utils.make_grid(\n",
    "    numpy_to_torch(CIFAR_images), nrow=4, normalize=True, pad_value=0.9\n",
    ")\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Image examples of the CIFAR10 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"outputs/ViT-image-examples.png\", dpi=200)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2366bcd-bbde-494d-8752-8370ed5bfca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_patches = img_to_patch(CIFAR_images, patch_size=4, flatten_channels=False)\n",
    "\n",
    "fig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14, 3))\n",
    "fig.suptitle(\"Images as input sequences of patches\")\n",
    "for i in range(CIFAR_images.shape[0]):\n",
    "    img_grid = torchvision.utils.make_grid(\n",
    "        numpy_to_torch(img_patches[i]), nrow=64, normalize=True, pad_value=0.9\n",
    "    )\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "    ax[i].imshow(img_grid)\n",
    "    ax[i].axis(\"off\")\n",
    "#  plt.show()\n",
    "\n",
    "plt.savefig(\"outputs/ViT-patches.png\", dpi=200)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5da2f11-f3d5-4101-be1d-b010a3ee2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        CHECKPOINT_PATH,\n",
    "        exmp_imgs,\n",
    "        lr=1e-3,\n",
    "        weight_decay=0.01,\n",
    "        seed=42,\n",
    "        **model_hparams\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Module for summarizing all training functionalities for classification on CIFAR10.\n",
    "\n",
    "        Inputs:\n",
    "            exmp_imgs - Example imgs, used as input to initialize the model\n",
    "            lr - Learning rate of the optimizer to use\n",
    "            weight_decay - Weight decay to use in the optimizer\n",
    "            seed - Seed to use in the model initialization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.seed = seed\n",
    "        self.rng = jax.random.PRNGKey(self.seed)\n",
    "\n",
    "        self.model = model(**model_hparams)\n",
    "        self.CHECKPOINT_PATH = CHECKPOINT_PATH\n",
    "\n",
    "        self.log_dir = os.path.join(self.CHECKPOINT_PATH, \"logs\")\n",
    "        self.logger = SummaryWriter(log_dir=self.log_dir)\n",
    "\n",
    "        self.create_functions()\n",
    "\n",
    "        self.init_model(exmp_imgs)\n",
    "        self.loss_log = []\n",
    "        self.metric_log = []\n",
    "\n",
    "    def create_functions(self):\n",
    "        def calculate_loss(params, rng, batch, train):\n",
    "            imgs, labels = batch\n",
    "            labels_onehot = jax.nn.one_hot(labels, num_classes=self.model.num_classes)\n",
    "            rng, dropout_apply_rng = random.split(rng)\n",
    "            logits = self.model.apply(\n",
    "                {\"params\": params},\n",
    "                imgs,\n",
    "                train=train,\n",
    "                rngs={\"dropout\": dropout_apply_rng},\n",
    "            )\n",
    "            loss = optax.softmax_cross_entropy(logits, labels_onehot).mean()\n",
    "            acc = (logits.argmax(axis=-1) == labels).mean()\n",
    "            return loss, (acc, rng)\n",
    "\n",
    "        def train_step(state, rng, batch):\n",
    "            def loss_fn(params):\n",
    "                return calculate_loss(params, rng, batch, train=True)\n",
    "\n",
    "            (loss, (acc, rng)), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n",
    "                state.params\n",
    "            )\n",
    "\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "            return state, rng, loss, acc\n",
    "\n",
    "        def eval_step(state, rng, batch):\n",
    "\n",
    "            _, (acc, rng) = calculate_loss(state.params, rng, batch, train=False)\n",
    "            return rng, acc\n",
    "\n",
    "        self.train_step = jax.jit(train_step)\n",
    "        self.eval_step = jax.jit(eval_step)\n",
    "\n",
    "    def init_model(self, exmp_imgs):\n",
    "\n",
    "        self.rng, init_rng, dropout_init_rng = random.split(self.rng, 3)\n",
    "        self.init_params = self.model.init(\n",
    "            {\"params\": init_rng, \"dropout\": dropout_init_rng}, exmp_imgs, train=True\n",
    "        )[\"params\"]\n",
    "        self.state = None\n",
    "\n",
    "    def init_optimizer(self, num_epochs, num_steps_per_epoch):\n",
    "\n",
    "        lr_schedule = optax.piecewise_constant_schedule(\n",
    "            init_value=self.lr,\n",
    "            boundaries_and_scales={\n",
    "                int(num_steps_per_epoch * num_epochs * 0.6): 0.1,\n",
    "                int(num_steps_per_epoch * num_epochs * 0.85): 0.1,\n",
    "            },\n",
    "        )\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(1.0),  # Clip gradients at norm 1\n",
    "            optax.adamw(lr_schedule, weight_decay=self.weight_decay),\n",
    "        )\n",
    "\n",
    "        self.state = train_state.TrainState.create(\n",
    "            apply_fn=self.model.apply,\n",
    "            params=self.init_params if self.state is None else self.state.params,\n",
    "            tx=optimizer,\n",
    "        )\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, num_epochs=200,graph_progress=None):\n",
    "        self.train_loader = train_loader\n",
    "\n",
    "        self.init_optimizer(num_epochs, len(self.train_loader))\n",
    "\n",
    "        best_eval = 0.0\n",
    "        tq = tqdm(range(1, num_epochs + 1))\n",
    "        for epoch_idx in tq:\n",
    "            self.train_epoch(epoch=epoch_idx)\n",
    "            if epoch_idx % 2 == 0:\n",
    "                eval_acc = self.eval_model(val_loader)\n",
    "                self.logger.add_scalar(\"val/acc\", eval_acc, global_step=epoch_idx)\n",
    "                tq.set_postfix({\"val/acc\": eval_acc})\n",
    "                if eval_acc >= best_eval:\n",
    "                    best_eval = eval_acc\n",
    "                    self.save_model(step=epoch_idx)\n",
    "                self.logger.flush()\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "\n",
    "        metrics = defaultdict(list)\n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\", leave=False):\n",
    "            self.state, self.rng, loss, acc = self.train_step(\n",
    "                self.state, self.rng, batch\n",
    "            )\n",
    "            metrics[\"loss\"].append(loss)\n",
    "            metrics[\"acc\"].append(acc)\n",
    "        for key in metrics:\n",
    "            avg_val = np.stack(jax.device_get(metrics[key])).mean()\n",
    "            self.logger.add_scalar(\"train/\" + key, avg_val, global_step=epoch)\n",
    "\n",
    "    def eval_model(self, data_loader):\n",
    "\n",
    "        correct_class, count = 0, 0\n",
    "        for batch in data_loader:\n",
    "            self.rng, acc = self.eval_step(self.state, self.rng, batch)\n",
    "            correct_class += acc * batch[0].shape[0]\n",
    "            count += batch[0].shape[0]\n",
    "        eval_acc = (correct_class / count).item()\n",
    "        return eval_acc\n",
    "\n",
    "    def save_model(self, step=0):\n",
    "\n",
    "        checkpoints.save_checkpoint(\n",
    "            ckpt_dir=self.log_dir, target=self.state.params, step=step, overwrite=True\n",
    "        )\n",
    "\n",
    "    def load_model(self, name=\"ViT.ckpt\", pretrained=False):\n",
    "\n",
    "        if not pretrained:\n",
    "            params = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=None)\n",
    "        else:\n",
    "            params = checkpoints.restore_checkpoint(\n",
    "                ckpt_dir=os.path.join(self.CHECKPOINT_PATH, name), target=None\n",
    "            )\n",
    "        self.state = train_state.TrainState.create(\n",
    "            apply_fn=self.model.apply,\n",
    "            params=params,\n",
    "            tx=self.state.tx if self.state else optax.adamw(self.lr),\n",
    "        )\n",
    "\n",
    "    def checkpoint_exists(self, name=\"ViT.ckpt\"):\n",
    "        return os.path.isfile(os.path.join(self.CHECKPOINT_PATH, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87cef037-ff3d-4c7d-9a10-9f7694bff965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL TRAINING\n",
    "def train_model(*args, num_epochs=200, retrain=False,graph_progress=None, **kwargs):\n",
    "    trainer = TrainerModule(*args, **kwargs)\n",
    "    if not trainer.checkpoint_exists() or retrain == True:\n",
    "        print(\"Training\")\n",
    "        trainer.train_model(train_loader, val_loader, num_epochs=num_epochs, graph_progress=graph_progress)\n",
    "        trainer.load_model()\n",
    "    else:\n",
    "        print(\"Skipping training\")\n",
    "        trainer.load_model(pretrained=True)\n",
    "    val_acc = trainer.eval_model(val_loader)\n",
    "    test_acc = trainer.eval_model(test_loader)\n",
    "    return trainer, {\"val\": val_acc, \"test\": test_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4297f709-5a33-4c2f-8e6a-0634d209adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0f9d095ffa455fb688078a26f2d3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c236133eac4225a632e065e9303c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexmp_imgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVisionTransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT results\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs, retrain, graph_progress, *args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mcheckpoint_exists() \u001b[38;5;129;01mor\u001b[39;00m retrain \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mload_model()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mTrainerModule.train_model\u001b[0;34m(self, train_loader, val_loader, num_epochs, graph_progress)\u001b[0m\n\u001b[1;32m    107\u001b[0m tq \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m tq:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    111\u001b[0m         eval_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_model(val_loader)\n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mTrainerModule.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    121\u001b[0m metrics \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng, loss, acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m    127\u001b[0m     metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorcher/lib/python3.10/site-packages/flax/core/frozen_dict.py:159\u001b[0m, in \u001b[0;36mFrozenDict.tree_unflatten\u001b[0;34m(cls, _, data)\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[38;5;124;03m\"\"\"Flattens this FrozenDict.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    A flattened version of this FrozenDict instance.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict,), ()\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_unflatten\u001b[39m(\u001b[38;5;28mcls\u001b[39m, _, data):\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;66;03m# data is already deep copied due to tree map mechanism\u001b[39;00m\n\u001b[1;32m    162\u001b[0m   \u001b[38;5;66;03m# we can skip the deep copy in the constructor\u001b[39;00m\n\u001b[1;32m    163\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39mdata, __unsafe_skip_copy__\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, results = train_model(\n",
    "    exmp_imgs=next(iter(train_loader))[0],\n",
    "    embed_dim=256,\n",
    "    hidden_dim=512,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    patch_size=4,\n",
    "    num_channels=3,\n",
    "    num_patches=64,\n",
    "    num_classes=10,\n",
    "    dropout_prob=0.2,\n",
    "    lr=3e-4,\n",
    "    retrain=True,\n",
    "    num_epochs=10,\n",
    "    CHECKPOINT_PATH=CHECKPOINT_PATH,\n",
    "    model=VisionTransformer,\n",
    "    graph_progress=10\n",
    ")\n",
    "print(\"ViT results\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1878d6-b790-496d-8469-cf8237297ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

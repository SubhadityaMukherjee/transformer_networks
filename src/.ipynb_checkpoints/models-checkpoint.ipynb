{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cbbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.set_cmap(\"cividis\")\n",
    "from IPython.display import set_matplotlib_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec58cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_matplotlib_formats(\"svg\", \"pdf\")\n",
    "import matplotlib\n",
    "from matplotlib.colors import to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e907c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d83654",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2662b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from .blocks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e51fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    embed_dim: int\n",
    "    hidden_dim: int\n",
    "    num_heads: int\n",
    "    num_channels: int\n",
    "    num_layers: int\n",
    "    num_classes: int\n",
    "    patch_size: int\n",
    "    num_patches: int\n",
    "    dropout_prob: float = 0.0\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        self.input_layer = nn.Dense(self.embed_dim)\n",
    "        self.transformer = [\n",
    "            AttentionBlock(\n",
    "                self.embed_dim, self.hidden_dim, self.num_heads, self.dropout_prob\n",
    "            )\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "        self.mlp_head = nn.Sequential([nn.LayerNorm(), nn.Dense(self.num_classes)])\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "        self.cls_token = self.param(\n",
    "            \"cls_token\", nn.initializers.normal(stddev=1.0), (1, 1, self.embed_dim)\n",
    "        )\n",
    "        self.pos_embedding = self.param(\n",
    "            \"pos_embedding\",\n",
    "            nn.initializers.normal(stddev=1.0),\n",
    "            (1, 1 + self.num_patches, self.embed_dim),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        cls_token = self.cls_token.repeat(B, axis=0)\n",
    "        x = jnp.concatenate([cls_token, x], axis=1)\n",
    "        x = x + self.pos_embedding[:, : T + 1]\n",
    "\n",
    "        x = self.dropout(x, deterministic=not train)\n",
    "        for attn_block in self.transformer:\n",
    "            x = attn_block(x, train=train)\n",
    "\n",
    "        cls = x[:, 0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Transformers and Other Stuff

- A nice little roadmap to Transformers
- Starting with RNNs, GRU, LSTMs, BiDirectionalRecurrrent models and then moving on to attention etc
- Fully in Pytorch so far. Jax might be added at some point
- Tested on the tiny-shakespeare dataset.
- Further notes can be found at [my_blog](https://subhadityamukherjee.github.io/AI-knowledge-base/#Basic%20RNN%20Architectures/#basic-rnn-architectures)
# TALK!
- If you are coming from the TFUG Talk on Jun27th 2022 , open vision_transformers/talk for the notebook/slides

# How to run?
- Just see runner.py 
- Uncomment the network you need
- python3 runner.py --epochs 1000 --lr 0.01
- This is still a work in progress so this will likely be optimized quite a lot in the coming days

## References
- https://github.com/georgeyiasemis/Recurrent-Neural-Networks-from-scratch-using-PyTorch
- https://eli.thegreenplace.net/2018/backpropagation-through-a-fully-connected-layer/
- https://github.com/christianversloot/machine-learning-articles/blob/main/from-vanilla-rnns-to-transformers-a-history-of-seq2seq-learning.md
- https://github.com/patrick-kidger/equinox/tree/main/equinox/nn
- https://github.com/hyunwoongko/transformer
- https://einops.rocks/
- https://github.com/sooftware/attentions
